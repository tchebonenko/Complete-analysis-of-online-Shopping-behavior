{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem statement:\n",
    "Consider you have training data (with the 'Revenue' attribute) for records from June-Sept only. For all records from Oct-Dec, the 'Revenue' attribute is missing. Build a semi-supervised self labelling model to estimate 'Revenue' for the missing records in Oct-Dec and then fit your classifier. Report classification performance on Feb-March data set with and without the self-labelled data.\n",
    "\n",
    "1. If you dont consider the records from Oct-Dec, generate the classification performance on Test data\n",
    "2. After using the self labelled data and training data together, does the classification performance on \n",
    "Test data improve? Comment on which metrics are of importance here.\n",
    "\n",
    "# Download data and import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "A5GJu4TdSHK-"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import make_pipeline, make_union\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from tpot.builtins import StackingEstimator\n",
    "from tpot.export_utils import set_param_recursive\n",
    "\n",
    "from tpot import TPOTClassifier\n",
    "from sklearn.semi_supervised import LabelSpreading\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Administrative</th>\n",
       "      <th>Administrative_Duration</th>\n",
       "      <th>Informational</th>\n",
       "      <th>Informational_Duration</th>\n",
       "      <th>ProductRelated_Duration</th>\n",
       "      <th>ExitRates</th>\n",
       "      <th>PageValues</th>\n",
       "      <th>Month</th>\n",
       "      <th>Revenue</th>\n",
       "      <th>SpecialDay_0.2</th>\n",
       "      <th>...</th>\n",
       "      <th>TrafficType_3</th>\n",
       "      <th>TrafficType_4</th>\n",
       "      <th>TrafficType_5</th>\n",
       "      <th>TrafficType_6</th>\n",
       "      <th>TrafficType_7</th>\n",
       "      <th>TrafficType_8</th>\n",
       "      <th>TrafficType_9</th>\n",
       "      <th>VisitorType_Other</th>\n",
       "      <th>VisitorType_Returning_Visitor</th>\n",
       "      <th>Weekend_True</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.666667</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>627.500000</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 63 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Administrative  Administrative_Duration  Informational  \\\n",
       "0               0                      0.0              0   \n",
       "1               0                      0.0              0   \n",
       "2               0                      0.0              0   \n",
       "3               0                      0.0              0   \n",
       "4               0                      0.0              0   \n",
       "\n",
       "   Informational_Duration  ProductRelated_Duration  ExitRates  PageValues  \\\n",
       "0                     0.0                 0.000000       0.20         0.0   \n",
       "1                     0.0                64.000000       0.10         0.0   \n",
       "2                     0.0                 0.000000       0.20         0.0   \n",
       "3                     0.0                 2.666667       0.14         0.0   \n",
       "4                     0.0               627.500000       0.05         0.0   \n",
       "\n",
       "   Month  Revenue  SpecialDay_0.2  ...  TrafficType_3  TrafficType_4  \\\n",
       "0      2    False               0  ...              0              0   \n",
       "1      2    False               0  ...              0              0   \n",
       "2      2    False               0  ...              1              0   \n",
       "3      2    False               0  ...              0              1   \n",
       "4      2    False               0  ...              0              1   \n",
       "\n",
       "   TrafficType_5  TrafficType_6  TrafficType_7  TrafficType_8  TrafficType_9  \\\n",
       "0              0              0              0              0              0   \n",
       "1              0              0              0              0              0   \n",
       "2              0              0              0              0              0   \n",
       "3              0              0              0              0              0   \n",
       "4              0              0              0              0              0   \n",
       "\n",
       "   VisitorType_Other  VisitorType_Returning_Visitor  Weekend_True  \n",
       "0                  0                              1             0  \n",
       "1                  0                              1             0  \n",
       "2                  0                              1             0  \n",
       "3                  0                              1             0  \n",
       "4                  0                              1             1  \n",
       "\n",
       "[5 rows x 63 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load data    \n",
    "with open('./transfer_files/df_data.pickle', 'rb') as f:\n",
    "    df_data = pickle.load(f)\n",
    "df_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "AkJuoR1FX7N5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6523, 2035)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Split data on train and test (Train data entries corresponding to the months of June-Dec, and test data entries corresponding to Feb-March.)\n",
    "df_train = df_data[df_data['Month'] >= 6]\n",
    "df_test = df_data[(df_data['Month'] >= 2) & (df_data['Month'] <= 3)]\n",
    "len(df_train), len(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pv_selected_columns = ['ProductRelated_Duration', 'PageValues', 'Browser_12', 'Browser_2',\n",
    "                         'Region_8', 'Region_9', 'TrafficType_14', 'TrafficType_18', 'TrafficType_19',\n",
    "                         'TrafficType_3', 'TrafficType_6', 'TrafficType_7',\n",
    "                         'VisitorType_Returning_Visitor', 'Weekend_True']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reduced train data classification performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1539, 63)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_reduced_train = df_train[(df_train['Month'] < 10)].copy()\n",
    "df_reduced_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1539, 14) (1539,)\n",
      "(2035, 14) (2035,)\n"
     ]
    }
   ],
   "source": [
    "X_train_reduced = df_reduced_train[pv_selected_columns].values\n",
    "y_train_reduced = df_reduced_train['Revenue'].values\n",
    "\n",
    "X_test = df_test[pv_selected_columns].values\n",
    "y_test = df_test['Revenue'].values\n",
    "\n",
    "print(X_train_reduced.shape, y_train_reduced.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Standardize features by removing the mean and scaling to unit variance\n",
    "scaler = StandardScaler()\n",
    "X_train_reduced_st = scaler.fit(X_train_reduced).transform(X_train_reduced)\n",
    "X_test_st = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Optimization Progress:   0%|          | 0/1300 [00:00<?, ?pipeline/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generation 1 - Current best internal CV score: 0.5901266980888318\n",
      "\n",
      "Generation 2 - Current best internal CV score: 0.5989636123476292\n",
      "\n",
      "Generation 3 - Current best internal CV score: 0.6021539820917374\n",
      "\n",
      "Generation 4 - Current best internal CV score: 0.6021539820917374\n",
      "\n",
      "Generation 5 - Current best internal CV score: 0.6021539820917374\n",
      "\n",
      "Generation 6 - Current best internal CV score: 0.6061921654665279\n",
      "\n",
      "Generation 7 - Current best internal CV score: 0.6061921654665279\n",
      "\n",
      "Generation 8 - Current best internal CV score: 0.6061921654665279\n",
      "\n",
      "Generation 9 - Current best internal CV score: 0.6061921654665279\n",
      "\n",
      "Generation 10 - Current best internal CV score: 0.6063011631130151\n",
      "\n",
      "Generation 11 - Current best internal CV score: 0.6082604949867364\n",
      "\n",
      "Generation 12 - Current best internal CV score: 0.6082604949867364\n",
      "\n",
      "Generation 13 - Current best internal CV score: 0.6082604949867364\n",
      "\n",
      "Generation 14 - Current best internal CV score: 0.612523832202199\n",
      "\n",
      "Generation 15 - Current best internal CV score: 0.612523832202199\n",
      "\n",
      "Generation 16 - Current best internal CV score: 0.612523832202199\n",
      "\n",
      "Generation 17 - Current best internal CV score: 0.612523832202199\n",
      "\n",
      "Generation 18 - Current best internal CV score: 0.612523832202199\n",
      "\n",
      "Generation 19 - Current best internal CV score: 0.612523832202199\n",
      "\n",
      "Generation 20 - Current best internal CV score: 0.612523832202199\n",
      "\n",
      "Generation 21 - Current best internal CV score: 0.612523832202199\n",
      "\n",
      "Generation 22 - Current best internal CV score: 0.612523832202199\n",
      "\n",
      "Generation 23 - Current best internal CV score: 0.612523832202199\n",
      "\n",
      "Generation 24 - Current best internal CV score: 0.612523832202199\n",
      "\n",
      "Generation 25 - Current best internal CV score: 0.612523832202199\n",
      "\n",
      "Best pipeline: MultinomialNB(CombineDFs(MultinomialNB(input_matrix, alpha=100.0, fit_prior=True), DecisionTreeClassifier(MultinomialNB(input_matrix, alpha=1.0, fit_prior=False), criterion=entropy, max_depth=1, min_samples_leaf=3, min_samples_split=18)), alpha=1.0, fit_prior=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TPOTClassifier(generations=25, n_jobs=-1, population_size=50, random_state=2,\n",
       "               scoring='f1', verbosity=2)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#1. If you dont consider the records from Oct-Dec, generate the classification performance on Test data\n",
    "tpot = TPOTClassifier(generations=25, \n",
    "                      population_size=50, \n",
    "                      scoring='f1',\n",
    "                      verbosity=2,\n",
    "                      random_state = 2,\n",
    "                      n_jobs=-1)\n",
    "tpot.fit(X_train_reduced, y_train_reduced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import numpy as np\n",
      "import pandas as pd\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.naive_bayes import MultinomialNB\n",
      "from sklearn.pipeline import make_pipeline, make_union\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from tpot.builtins import StackingEstimator\n",
      "from tpot.export_utils import set_param_recursive\n",
      "\n",
      "# NOTE: Make sure that the outcome column is labeled 'target' in the data file\n",
      "tpot_data = pd.read_csv('PATH/TO/DATA/FILE', sep='COLUMN_SEPARATOR', dtype=np.float64)\n",
      "features = tpot_data.drop('target', axis=1)\n",
      "training_features, testing_features, training_target, testing_target = \\\n",
      "            train_test_split(features, tpot_data['target'], random_state=2)\n",
      "\n",
      "# Average CV score on the training set was: 0.612523832202199\n",
      "exported_pipeline = make_pipeline(\n",
      "    make_union(\n",
      "        StackingEstimator(estimator=MultinomialNB(alpha=100.0, fit_prior=True)),\n",
      "        StackingEstimator(estimator=make_pipeline(\n",
      "            StackingEstimator(estimator=MultinomialNB(alpha=1.0, fit_prior=False)),\n",
      "            DecisionTreeClassifier(criterion=\"entropy\", max_depth=1, min_samples_leaf=3, min_samples_split=18)\n",
      "        ))\n",
      "    ),\n",
      "    MultinomialNB(alpha=1.0, fit_prior=True)\n",
      ")\n",
      "# Fix random state for all the steps in exported pipeline\n",
      "set_param_recursive(exported_pipeline.steps, 'random_state', 2)\n",
      "\n",
      "exported_pipeline.fit(training_features, training_target)\n",
      "results = exported_pipeline.predict(testing_features)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(tpot.export())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model performance:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.97      0.97      0.97      1855\n",
      "        True       0.67      0.69      0.68       180\n",
      "\n",
      "    accuracy                           0.94      2035\n",
      "   macro avg       0.82      0.83      0.82      2035\n",
      "weighted avg       0.94      0.94      0.94      2035\n",
      "\n",
      "Confusion matrix\n",
      "[[1793   62]\n",
      " [  55  125]]\n"
     ]
    }
   ],
   "source": [
    "# Average CV score on the training set was: 0.609134780563352\n",
    "exported_pipeline = make_pipeline(\n",
    "    StackingEstimator(estimator=DecisionTreeClassifier(criterion=\"entropy\", max_depth=4, min_samples_leaf=5, min_samples_split=2)),\n",
    "    MultinomialNB(alpha=10.0, fit_prior=False)\n",
    ")\n",
    "# Fix random state for all the steps in exported pipeline\n",
    "set_param_recursive(exported_pipeline.steps, 'random_state', 2)\n",
    "\n",
    "\n",
    "exported_pipeline.fit(X_train_reduced, y_train_reduced)\n",
    "results = exported_pipeline.predict(X_test)\n",
    "\n",
    "cm = confusion_matrix(y_test, results)\n",
    "print(\"Model performance:\")\n",
    "print(classification_report(y_test, results))\n",
    "print(\"Confusion matrix\")\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply label spreading on the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6523, 14) (6523,) (4984, 14) (1539, 63)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-1    4984\n",
       " 0    1296\n",
       " 1     243\n",
       "Name: Revenue, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_unlabeled_train = df_train[(df_train['Month'] >= 10)].copy()\n",
    "df_unlabeled_train['Revenue'] = -1\n",
    "df_reduced_train['Revenue'] = df_reduced_train['Revenue'].apply(lambda x: 1 if x==True else 0)\n",
    "\n",
    "df_X = pd.concat([df_reduced_train, df_unlabeled_train]) \n",
    "X = df_X[pv_selected_columns].values\n",
    "y = df_X['Revenue'].values\n",
    "unlabeled_set = df_unlabeled_train[pv_selected_columns].values\n",
    "\n",
    "print(X.shape, y.shape, unlabeled_set.shape, df_reduced_train.shape)\n",
    "df_X['Revenue'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LabelSpreading(gamma=0.25, max_iter=20)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Initialize the LabelSpreading model\n",
    "lp_model = LabelSpreading(gamma=.25, max_iter=20)\n",
    "lp_model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4984,)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract the label predictions for the unlabeled data\n",
    "predicted_labels = lp_model.transduction_[1539:]\n",
    "predicted_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    4499\n",
       "True      485\n",
       "Name: Revenue, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_unlabeled_train['Revenue'] = predicted_labels\n",
    "df_unlabeled_train['Revenue'] = df_unlabeled_train['Revenue'].apply(lambda x: True if x == 1 else False)\n",
    "df_unlabeled_train['Revenue'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_spreaded = pd.concat([df_reduced_train, df_unlabeled_train]) \n",
    "X_train_spreaded = df_train_spreaded[pv_selected_columns].values\n",
    "y_train_spreaded = df_train_spreaded['Revenue'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model performance:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.97      0.97      0.97      1855\n",
      "        True       0.71      0.69      0.70       180\n",
      "\n",
      "    accuracy                           0.95      2035\n",
      "   macro avg       0.84      0.83      0.84      2035\n",
      "weighted avg       0.95      0.95      0.95      2035\n",
      "\n",
      "Confusion matrix\n",
      "[[1804   51]\n",
      " [  55  125]]\n"
     ]
    }
   ],
   "source": [
    "exported_pipeline = make_pipeline(\n",
    "    StackingEstimator(estimator=DecisionTreeClassifier(criterion=\"entropy\", max_depth=4, min_samples_leaf=5, min_samples_split=2)),\n",
    "    MultinomialNB(alpha=10.0, fit_prior=False)\n",
    ")\n",
    "\n",
    "set_param_recursive(exported_pipeline.steps, 'random_state', 2)\n",
    "\n",
    "\n",
    "exported_pipeline.fit(X_train_spreaded, y_train_spreaded)\n",
    "results = exported_pipeline.predict(X_test)\n",
    "\n",
    "cm = confusion_matrix(y_test, results)\n",
    "print(\"Model performance:\")\n",
    "print(classification_report(y_test, results))\n",
    "print(\"Confusion matrix\")\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**F1-score for Revenue=True inproved from 0.68 to 0.70**\n",
    "\n",
    "**Note:**\n",
    "I did not use Recall (which is more apropriate here) since AutoML was giving me perfect scores in the first step and there was nothing to improve."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "nCXFHXr8Qjs_",
    "0tpS8JDKQjs_",
    "SGjBlxpAQjtB"
   ],
   "name": "ML_for_Data_Science 20210506.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "329.797px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
